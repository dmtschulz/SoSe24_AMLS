{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Acquisition and Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from rasterio.plot import show\n",
    "\n",
    "from acquisition_alignment.dl_functions import download_osm, get_building_bbox, download_sentinel2_images_openeo\n",
    "from acquisition_alignment.connect_openeo import connect_to_openeo\n",
    "from acquisition_alignment.plt_functions import plot_city_buildings, rgb_image, single_band_image, overlay_image, irb_image, stretch_contrast, get_epsg_from_tif, reproject_gdf\n",
    "\n",
    "from preparation.preparation import create_building_mask, extract_patches\n",
    "from preparation.gen_dataset import save_patches_and_masks, pair_files, split_and_save\n",
    "\n",
    "from modeling_tuning.build_dataset import BuildingDataset\n",
    "from modeling_tuning.build_classifier import BuildingClassifierBaseline, BuildingClassifierUnet\n",
    "from modeling_tuning.earlystopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through 10 'big' cities and download OSM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyrosm_path = f\"acquisition_alignment/pyrosm_cities/\"\n",
    "os.makedirs(pyrosm_path, exist_ok=True)\n",
    "\n",
    "# List of 10 'big' cities + Test city\n",
    "city_names = [\"Berlin\", \"Bonn\", \"Muenchen\", \"Koeln\", \"Frankfurt\", \"Stuttgart\", \"Dortmund\", \"Duesseldorf\", \"Bremen\", \"Leipzig\", \"Dresden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 min\n",
    "# Loop through each city\n",
    "for city in city_names:\n",
    "    #Download and process OSM data\n",
    "    download_osm(city, pyrosm_path)\n",
    "    print(f\"Download of {city} done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load buildings and bboxes of the cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save shape files (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_shapefile(buildings_gdf, bbox, city_name):\n",
    "    # Define the path to save files\n",
    "    files_path = \"acquisition_alignment/pyrosm_cities/buildings/\"\n",
    "    os.makedirs(files_path, exist_ok=True)\n",
    "    \n",
    "    # Save buildings GeoDataFrame to shapefile\n",
    "    buildings_gdf.to_file(files_path+f\"{city_name}_buildings.shp\")\n",
    "    \n",
    "    # Save bbox to JSON file\n",
    "    north, south, west, east = bbox\n",
    "    bbox_dict = {\n",
    "        \"north\": north,\n",
    "        \"south\": south,\n",
    "        \"west\": west,\n",
    "        \"east\": east\n",
    "    }\n",
    "    \n",
    "    with open(files_path + f\"{city_name}_bbox.json\", \"w\") as json_file:\n",
    "        json.dump(bbox_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract buildings and bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Berlin.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Berlin converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Berlin done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Bonn.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Bonn converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Bonn done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Muenchen.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Muenchen  converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Muenchen  done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Koeln.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Koeln converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Koeln done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Frankfurt.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Frankfurt converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Frankfurt done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Stuttgart.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Stuttgart converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Stuttgart done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Dortmund.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Dortmund converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Dortmund done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Duesseldorf.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Duesseldorf converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Duesseldorf done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Bremen.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Bremen converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Bremen done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Leipzig.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Leipzig converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Leipzig done.\n",
      "\n",
      "c:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\pyrosm_cities\\Dresden.osm.pbf\n",
      "OSM data get_buildings done.\n",
      "OSM for Dresden converted into GeoDataFrame, with crs=EPSG:4326 done.\n",
      "OSM data get_boundaries done.\n",
      "Get buildings and boundaries of Dresden done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~11 min\n",
    "cities_data = {}\n",
    "for city in city_names:\n",
    "    buildings_gdf, bbox = get_building_bbox(pyrosm_path, city)\n",
    "    # Saving buildings_gdf to a shapefile\n",
    "    # save_to_shapefile(buildings_gdf, bbox, city)\n",
    "    cities_data[city] = [buildings_gdf, bbox]\n",
    "    print(f\"Get buildings and boundaries of {city} done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Satellite images (.tif files), save plots and images (rgb and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connect to openeo.dataspace.copernicus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to OpenEO...\n",
      "Authenticated using refresh token.\n",
      "Connected to OpenEO.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect to OpenEO\n",
    "connection = connect_to_openeo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download Sentinel 2 L2a Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Sentinel-2 L2a images from OpenEO...\n",
      "0:00:00 Job 'j-240618fefe0f4162807155ddc6b3857b': send 'start'\n",
      "0:00:20 Job 'j-240618fefe0f4162807155ddc6b3857b': queued (progress 0%)\n",
      "0:00:26 Job 'j-240618fefe0f4162807155ddc6b3857b': queued (progress 0%)\n",
      "0:00:33 Job 'j-240618fefe0f4162807155ddc6b3857b': queued (progress 0%)\n",
      "0:00:55 Job 'j-240618fefe0f4162807155ddc6b3857b': queued (progress 0%)\n",
      "0:01:05 Job 'j-240618fefe0f4162807155ddc6b3857b': queued (progress 0%)\n",
      "0:01:18 Job 'j-240618fefe0f4162807155ddc6b3857b': queued (progress 0%)\n",
      "0:01:34 Job 'j-240618fefe0f4162807155ddc6b3857b': running (progress N/A)\n",
      "0:01:53 Job 'j-240618fefe0f4162807155ddc6b3857b': running (progress N/A)\n",
      "0:02:17 Job 'j-240618fefe0f4162807155ddc6b3857b': running (progress N/A)\n",
      "0:02:47 Job 'j-240618fefe0f4162807155ddc6b3857b': running (progress N/A)\n",
      "0:03:25 Job 'j-240618fefe0f4162807155ddc6b3857b': running (progress N/A)\n",
      "0:04:11 Job 'j-240618fefe0f4162807155ddc6b3857b': finished (progress 100%)\n",
      "Sentinel-2 images downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# 30 - 40 min\n",
    "dates_interval = [\"2021-06-01T12:00:00Z\", \"2021-07-30T12:01:00Z\"]\n",
    "cloud_cover_percentage = 6\n",
    "\n",
    "for city in city_names:\n",
    "    openeo_path = f\"acquisition_alignment/openeo_cities/{city}/\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(openeo_path, exist_ok=True)\n",
    "\n",
    "    # Download Sentinel-2 L2a images\n",
    "    _, bbox = cities_data[city]\n",
    "    download_sentinel2_images_openeo(connection, bbox, dates_interval, cloud_cover_percentage, openeo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buildings Plot and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (a) Buildings from OpenStreetMaps.\n",
      "1 (b) RGB Bands from Sentinel 2.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "band index 0 out of range (not in (1, 2, 3, 4))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m plot_city_buildings(city, buildings_gdf, images_save_dir)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 (b) RGB Bands from Sentinel 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mrgb_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentinel2_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_save_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2 (a) Single Band.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m single_band_image(sentinel2_path, images_save_dir, band_index)\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\TU\\AMLS\\acquisition_alignment\\plt_functions.py:43\u001b[0m, in \u001b[0;36mrgb_image\u001b[1;34m(sentinel2_path, output_image_path)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrgb_image\u001b[39m(sentinel2_path, output_image_path):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Load the Sentinel-2 image\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m rasterio\u001b[38;5;241m.\u001b[39mopen(sentinel2_path) \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;66;03m# Read the Red, Green, and Blue bands by their indices (B04, B03, B02)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m         red \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# * 0.0001 # B04\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         green \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# * 0.0001  # B03\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         blue \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# * 0.0001  # B02\u001b[39;00m\n",
      "File \u001b[1;32mrasterio\\\\_io.pyx:535\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: band index 0 out of range (not in (1, 2, 3, 4))"
     ]
    }
   ],
   "source": [
    "band_index = 1  # For single band. Band index to read (1 for B04, 2 for B03, 3 for B02, 4 for B08) (cmap=gray anyway)\n",
    "\n",
    "# Loop through each city in the dictionary and save image of the buildings of the city\n",
    "for city, data in cities_data.items():\n",
    "    # Path to the Sentinel-2 L2a image of a city\n",
    "    sentinel2_path = f\"acquisition_alignment/openeo_cities/{city_names[0]}/{city}.tif\"\n",
    "\n",
    "    buildings_gdf, _ = data\n",
    "\n",
    "    # Make directory for the images (for city). Ensure the directory exists\n",
    "    images_save_dir = f\"acquisition_alignment/images/{city}/\"\n",
    "    os.makedirs(images_save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"1 (a) Buildings from OpenStreetMaps.\")\n",
    "    plot_city_buildings(city, buildings_gdf, images_save_dir)\n",
    "\n",
    "    print(\"1 (b) RGB Bands from Sentinel 2.\")\n",
    "    rgb_image(sentinel2_path, images_save_dir)\n",
    "\n",
    "    print(\"2 (a) Single Band.\")\n",
    "    single_band_image(sentinel2_path, images_save_dir, band_index)\n",
    "\n",
    "    print(\"2 (b) Overlap Buildings from OSM onto Sentinel-2 image.\")\n",
    "    overlay_image(buildings_gdf, sentinel2_path, images_save_dir, band_index)\n",
    "\n",
    "    print(\"2 (c) IRB\")\n",
    "    irb_image(sentinel2_path, images_save_dir)\n",
    "    \n",
    "    break # One should be enough to be sure everything is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save mask for each city in a .tif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360, 1425, 4) Berlin\n",
      "(4884, 4080, 4) Bonn\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Patch extraction\n",
    "patch_size = 128  # Example patch size\n",
    "\n",
    "# Load the Sentinel-2 image\n",
    "for city, data in cities_data.items():\n",
    "    image_path = f\"acquisition_alignment/openeo_cities/{city}/{city}.tif\"  # Iterate over each of the 10 files\n",
    "    buildings_gdf, _ = data\n",
    "    satellite_crs = get_epsg_from_tif(image_path)\n",
    "    buildings_gdf = reproject_gdf(buildings_gdf, satellite_crs)\n",
    "\n",
    "    with rasterio.open(image_path) as src:\n",
    "        transform = src.transform\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "\n",
    "        # Read the image bands as integers. Bands are starting from 1 HERE (with rasterio)!\n",
    "        red = src.read(1).astype(np.float32)  # B04\n",
    "        green = src.read(2).astype(np.float32)  # B03\n",
    "        blue = src.read(3).astype(np.float32)  # B02\n",
    "        ir = src.read(4).astype(np.float32)  # B08\n",
    "\n",
    "        # Normalize each band to [0, 1] values\n",
    "        red = (red - np.min(red)) / (np.max(red) - np.min(red))\n",
    "        green = (green - np.min(green)) / (np.max(green) - np.min(green))\n",
    "        blue = (blue - np.min(blue)) / (np.max(blue) - np.min(blue))\n",
    "        ir = (ir - np.min(ir)) / (np.max(ir) - np.min(ir))\n",
    "\n",
    "        sentinel_image = np.dstack((red, green, blue, ir))  # [H, W, C]. Bands are NOW starting from 0!\n",
    "        print(sentinel_image.shape, city)\n",
    "\n",
    "        # Create the building mask\n",
    "        buildings_mask = create_building_mask(buildings_gdf, transform, width, height)\n",
    "\n",
    "        # Save the building mask as a new .tif file\n",
    "        mask_path = f'acquisition_alignment/openeo_cities/{city}/{city}_buildings_mask.tif'\n",
    "        with rasterio.open(\n",
    "            mask_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=buildings_mask.shape[0],\n",
    "            width=buildings_mask.shape[1],\n",
    "            count=1,\n",
    "            dtype=buildings_mask.dtype,  # use the same data type as the mask\n",
    "            crs=src.crs,\n",
    "            transform=transform,\n",
    "        ) as dst:\n",
    "            dst.write(buildings_mask, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load .tif mask files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in city_names:\n",
    "    mask_path = f'acquisition_alignment/openeo_cities/{city}/{city}_buildings_mask.tif'\n",
    "    # Load the building mask\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        buildings_mask = src.read(1)  # Mask is in the first band\n",
    "\n",
    "    # Check the unique values in the mask\n",
    "    unique_values = np.unique(buildings_mask)\n",
    "    print(\"Unique values in mask:\", unique_values)\n",
    "\n",
    "    # Display the mask with a grayscale colormap\n",
    "    show(buildings_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get patches from images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_image, patches_mask = extract_patches(sentinel_image, buildings_mask, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "tensor_image = torch.tensor(patches_image, dtype=torch.float32)\n",
    "tensor_mask = torch.tensor(patches_mask, dtype=torch.long)  # Masks are typically long integers in PyTorch\n",
    "\n",
    "# Ensure the dimensions are [N, H, W, C] for images and [N, H, W] for masks\n",
    "tensor_image = tensor_image.permute(0, 1, 2, 3)\n",
    "tensor_mask = tensor_mask.permute(0, 1, 2)\n",
    "\n",
    "# Output\n",
    "print(\"Image tensor shape:\", tensor_image.shape)\n",
    "print(\"Mask tensor shape:\", tensor_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at some of the patches and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_patches(tensor_image, tensor_mask, num_patches=10):\n",
    "    fig, axs = plt.subplots(num_patches, 2, figsize=(10, num_patches * 5))\n",
    "    for i in range(num_patches):\n",
    "        # Convert the tensor image to numpy and move the channels to the last dimension\n",
    "        img = tensor_image[i]\n",
    "        \n",
    "        # Stretch contrast for each channel (R, G, B, NIR)\n",
    "        img_stretched = np.zeros_like(img)\n",
    "        for c in range(img.shape[2]):\n",
    "            img_stretched[:, :, c] = stretch_contrast(img[:, :, c])\n",
    "        \n",
    "        # Normalize the stretched image for visualization\n",
    "        img_stretched = (img_stretched - np.min(img_stretched)) / (np.max(img_stretched) - np.min(img_stretched))\n",
    "        \n",
    "        # Display the image\n",
    "        axs[i, 0].imshow(img_stretched[:, :, :3])  # Display only RGB channels\n",
    "\n",
    "        axs[i, 0].set_title(f'Image Patch {i+1}')\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        # Display the mask\n",
    "        axs[i, 1].imshow(tensor_mask[i].numpy(), cmap='gray')\n",
    "        axs[i, 1].set_title(f'Mask Patch {i+1}')\n",
    "        axs[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first 10 patches\n",
    "visualize_patches(tensor_image, tensor_mask, num_patches=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all images and all patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images and labels\n",
    "output_directory = 'preparation/data/' # preparation/data/all_images*patches\n",
    "save_patches_and_masks(tensor_image, tensor_mask, output_directory, city, prefix='patch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make pairs: (patch, patch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'preparation/data/all_images'\n",
    "mask_dir = 'preparation/data/all_masks'\n",
    "# For splitting\n",
    "paired_files = pair_files(image_dir, mask_dir)\n",
    "print(len(paired_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into train and val and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'preparation/data'\n",
    "split_and_save(paired_files, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Modeling and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: in_channels, lr, weight_decay, num_epochs, batch_size, patience, min_delta, optimizer_type\n",
    "params_list = [\n",
    "    (3, 0.001, 1e-4, 10, 16, 5, 0.001, 'adam'),\n",
    "    (3, 0.001, 1e-4, 20, 16, 5, 0.001, 'sgd'),\n",
    "]\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BuildDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the train dataset\n",
    "train_image_dir = \"preparation/data/train/images\" \n",
    "train_mask_dir = \"preparation/data/train/masks\"\n",
    "train_dataset = BuildingDataset(train_image_dir, train_mask_dir, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "# Load the validation dataset\n",
    "val_image_dir = \"preparation/data/val/images\" \n",
    "val_mask_dir = \"preparation/data/val/masks\"\n",
    "val_dataset = BuildingDataset(val_image_dir, val_mask_dir, transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "# Load the test dataset (Berlin)\n",
    "# test_image_dir = \"preparation/data/test/images\"\n",
    "# test_mask_dir = \"preparation/data/test/masks\"\n",
    "# test_dataset = BuildingDataset(test_image_dir, test_mask_dir, transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_modelB(params):\n",
    "    print(f\"Params: {params}\")\n",
    "    in_channels, lr, weight_decay, num_epochs, batch_size, patience, min_delta, optimizer_type = params\n",
    "\n",
    "    model = BuildingClassifierBaseline(in_channels).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss with logits\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    # Select optimizer based on the parameter\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")\n",
    "\n",
    "    print(\"Training started\")\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
    "        for images, masks in train_loader_tqdm:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = outputs.squeeze(1)  # Remove channel dimension\n",
    "            masks = masks.squeeze(1)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix({\"Loss\": loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader_tqdm:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                outputs = outputs.squeeze(1)\n",
    "                masks = masks.squeeze(1)\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                val_loader_tqdm.set_postfix({\"Loss\": loss.item()})\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check early stopping condition\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = f'models/'\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    \n",
    "    model_name = f\"Baseline_{in_channels}_{lr}_{weight_decay}_{batch_size}_{optimizer_type}.pth\"\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_path, model_name))\n",
    "    print(f\"Model saved to {model_save_path + model_name}\")\n",
    "    \n",
    "    # Plot train and validation losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation Loss over Epochs for the Baseline Model')\n",
    "    plt.xticks(range(1, epoch + 1))  # Set x-axis ticks to integers only\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def run_in_parallel():\n",
    "    pool = Pool(processes=len(params_list))\n",
    "    pool.map(train_model, params_list)\n",
    "\n",
    "run_in_parallel()\n",
    "\"\"\"\n",
    "\n",
    "for p in params_list:\n",
    "    train_modelB(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, test_loader, m):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the model\n",
    "    if m == \"b\":\n",
    "        model = BuildingClassifierBaseline(in_channels=3)  # input channels\n",
    "    else:\n",
    "        model = BuildingClassifierUnet(in_channels=3)  # input channels\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss with logits\n",
    "\n",
    "    test_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            masks = masks.squeeze(1)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_masks.append(masks.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return np.concatenate(all_outputs), np.concatenate(all_masks)\n",
    "\n",
    "def compute_metrics(predicted_masks, true_masks, threshold=0.5):\n",
    "    predicted_masks = (predicted_masks > threshold).astype(int)\n",
    "    true_masks = true_masks.astype(int)\n",
    "    \n",
    "    # Flatten the masks to compute metrics\n",
    "    predicted_masks_flat = predicted_masks.flatten()\n",
    "    true_masks_flat = true_masks.flatten()\n",
    "    \n",
    "    accuracy = accuracy_score(true_masks_flat, predicted_masks_flat)\n",
    "    precision = precision_score(true_masks_flat, predicted_masks_flat)\n",
    "    recall = recall_score(true_masks_flat, predicted_masks_flat)\n",
    "    f1 = f1_score(true_masks_flat, predicted_masks_flat)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/Baseline_3_0.001_0.0001_16_adam.pth'\n",
    "predicted_masks, true_masks = evaluate_model(model_path, test_loader, \"b\")\n",
    "\n",
    "# Compute the metrics\n",
    "compute_metrics(predicted_masks, true_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visual Baseline one of the Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(test_loader, model_path, m, num_images=5):\n",
    "    # Load the model\n",
    "    if m == \"b\":\n",
    "        model = BuildingClassifierBaseline(in_channels=3)  # input channels\n",
    "    else:\n",
    "        model = BuildingClassifierUnet(in_channels=3)  # input channels\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, masks) in enumerate(test_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (preds > 0.5).astype(np.uint8)\n",
    "            \n",
    "            for i in range(min(num_images, images.size(0))):\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                \n",
    "                # Original image\n",
    "                axs[0].imshow(images[i].cpu().permute(1, 2, 0))\n",
    "                axs[0].set_title(\"Original Image\")\n",
    "                axs[0].axis(\"off\")\n",
    "                \n",
    "                # Ground truth mask\n",
    "                axs[1].imshow(masks[i].cpu().squeeze(), cmap=\"gray\")  # Squeeze to remove the channel dimension\n",
    "                axs[1].set_title(\"Ground Truth Mask\")\n",
    "                axs[1].axis(\"off\")\n",
    "                \n",
    "                # Predicted mask\n",
    "                axs[2].imshow(preds[i], cmap=\"gray\")  # Directly use preds[i] since it's already squeezed\n",
    "                axs[2].set_title(\"Predicted Mask\")\n",
    "                axs[2].axis(\"off\")\n",
    "                \n",
    "                plt.show()\n",
    "            \n",
    "            # Stop after visualizing num_images\n",
    "            if idx * test_loader.batch_size + i + 1 >= num_images:\n",
    "                break\n",
    "\n",
    "# Visualize the first 5 predictions in the test set\n",
    "visualize_predictions(test_loader, model_path, \"b\", num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = BuildingClassifierUnet(in_channels=3)  # Input channels = 3 (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_modelU(train_loader, val_loader, model, criterion, optimizer, device, num_epochs=10, patience=5, min_delta=0):\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
    "        for images, masks in train_loader_tqdm:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = outputs.squeeze(1)  # Remove channel dimension\n",
    "            masks = masks.squeeze(1)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix({\"Loss\": loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader_tqdm:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                outputs = outputs.squeeze(1)\n",
    "                masks = masks.squeeze(1)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                val_loader_tqdm.set_postfix({\"Loss\": loss.item()})\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Check early stopping condition\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    # Plot train and validation losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epoch + 2), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Unet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, criterion, optimizer\n",
    "model = BuildingClassifierUnet(in_channels=3).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_modelU(train_loader, val_loader, model, criterion, optimizer, device)\n",
    "\n",
    "# Save the model\n",
    "model_save_path = f'models/'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "torch.save(trained_model.state_dict(), model_save_path+f\"Unet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/Unet.pth'\n",
    "predicted_masks, true_masks = evaluate_model(model_path, test_loader, \"u\")\n",
    "\n",
    "# Compute the metrics\n",
    "compute_metrics(predicted_masks, true_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visual Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(test_loader, model_path, \"u\", num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
